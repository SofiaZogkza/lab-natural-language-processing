{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/7c43b8cn05l6gf4qv7v0gc800000gn/T/ipykernel_26279/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying SMS/emails as SPAM (scam) or HAM (legit)\n",
    "\n",
    "By Sofia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                             Will do.      0\n",
      "2    Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3    Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                  fyi      0\n",
      "..                                                 ...    ...\n",
      "995  So what's the latest? It sounds contradictory ...      0\n",
      "996  TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...      1\n",
      "997  Barb I will call to explain. Are you back in t...      0\n",
      "998    Yang on travelNot free tonite.May work tomorrow      0\n",
      "999  sbwhoeopSunday February 21 2010 7:42 PMHShaunH...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "(1000, 2)\n",
      "Columns:  Index(['text', 'label'], dtype='object')\n",
      "Legit like email: DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL I AM MIKE CHUKWU , THE MANAGER, BILLS AND EXCHANGE AT THE FOREIGN REMITTANCE DEPARTMENT OF THE ZENITH INTERNATIONAL BANK PLC. I AM WRITING THIS LETTER TO ASK FOR YOUR SUPPORT AND COOPERATION TO CARRY OUT THIS BUSINESS OPPORTUNITY IN MY DEPARTMENT. WE DISCOVERED AN ABANDONED SUM OF $15,000,000.00 (FIFTEEN MILLION UNITED STATES DOLLARS ONLY) IN AN ACCOUNT THAT BELONGS TO ONE OF OUR FOREIGN CUSTOMERS WHO DIED ALONG WITH HIS ENTIRE FAMILY OF A WIFE AND TWO CHILDREN IN NOVEMBER 1997 IN A PLANE CRASH. SINCE WE HEARD OF HIS DEATH, WE HAVE BEEN EXPECTING HIS NEXT-OF-KIN TO COME OVER AND PUT CLAIMS FOR HIS MONEY AS THE HEIR,BECAUSE WE CANNOT RELEASE THE FUND FROM HIS ACCOUNT UNLESS SOMEONE APPLIES FOR CLAIM AS THE NEXT-OF-KIN TO THE DECEASED AS INDICATED IN OUR BANKING GUIDELINES. UNFORTUNATELY, NEITHER THEIR FAMILY MEMBER NOR DISTANT RELATIVE HAS EVER APPEARED TO CLAIM THE SAID FUND. UPON THIS DISCOVERY,I AND OTHER OFFICIALS IN MY DEPARTMENT HAVE AGREED TO MAKE BUSINESS WITH YOU AND RELEASE THE TOTAL AMOUNT INTO YOUR ACCOUNT AS THE HEIR OF THE FUND SINCE NO ONE CAME FOR IT OR DISCOVERED HE MAINTAINED ACCOUNT WITH OUR BANK, OTHERWISE THE FUND WILL BE RETURNED TO THE BANKS TREASURY AS UNCLAIMED FUND. WE HAVE AGREED THAT OUR RATIO OF SHARING WILL BE AS STATED THUS; 30 % FOR YOU AS FOREIGN PARTNER, 60 % FOR US THE OFFICIALS IN MY DEPARTMENT AND 10% FOR THE SETTLEMENT OF ALL LOCAL AND FOREIGN EXPENCES INCURRED BY US AND YOU DURING THE COURSE OF THIS BUSINESS. UPON THE SUCCESSFUL COMPLETION OF THIS TRANSFER, I AND ONE OF MY COLLEAGUES WILL COME TO YOUR COUNTRY AND MIND OUR SHARE. IT IS FROM OUR 60 % WE INTEND TO IMPORT AGRICULTURAL MACHINERIES INTO MY COUNTRY AS A WAY OF RECYCLING THE FUND. TO COMMENCE THIS TRANSACTION, WE REQUIRE YOU TO IMMEDIATELY INDICATE YOUR INTEREST BY A RETURN E-MAIL AND ENCLOSE YOUR PRIVATE CONTACT TELEPHONE NUMBER, FAX NUMBER FULL NAME AND ADDRESS AND YOUR DESIGNATED BANK COORDINATES TO ENABLE US FILE LETTER OF CLAIM TO THE APPROPRIATE DEPARTMENTS FOR NECESSARY APPROVALS BEFORE THE TRANSFER CAN BE MADE. NOTE ALSO, THIS TRANSACTION MUST BE KEPT STRICTLY CONFIDENTIAL BECAUSE OF IT'S NATURE. I LOOK FORWARD TO RECEIVING YOUR PROMPT RESPONSE. REGARDS, MR. MIKE CHUKWU ZENITH INTERNATIONAL BANK PLC . \n",
      "Scam like email : Compliment, How are you doing today, Hope you have not forgotten me? I am Dr.Mr MichaelHoward,the man from Nigeria who contacted you some time ago to assist mesecure the release of some money accrued from the over Invoicing of a contract/inheritancethat was awarded by my (Government) some time ago during the military regime.Though you were not able to assist me conclude the transaction, I'm happyto inform you about my success in getting those funds transfered under theassistance and co-operation of a new partner from Brazil.Presently I'm inNetherlands for investment projects with my own share of the total sum. meanwhile, i didn't forget your past efforts and attempts to assist me intransferring those funds, I made sure you are not left out the benefit ofthe transaction hence I kept aside for you the sum of five Hundred ThousandUnited States Dollars.($500,000.00)Draft, the compensation also go for thegood work of God meaning for the orphans and disable in your country Pleasetake good care of them and the poor in needs also. I and my new partner agreed to compensate you with that amount for all yourpast efforts and attempts to assist me in this matter. I appreciate yourefforts at that time very much,so feel free and get in touch with my secretaryMr.Steven Chapman and instruct him on how to send the Bank draft to you.Please do let me know immediately you receive the Draft so that we can sharethe joy together after all the sufferings at that time. In the moment, I'mvery busy here because of the investment projects which me and my partnerare having at hands, finally,remember that i had left instruction to my secretaryso as soon as you contact him, he will send the Draft to you,so feel freeto get in touch with MY SECRETARY NOW. \n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/sofiazogkza/repos/Ironhack/Ironhack Lab/Week 4/D3/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)\n",
    "print(\"Columns: \",data.columns) # prints: Columns:  Index(['text'], dtype='object') so it has only 1 column\n",
    "print(\"Legit like email:\", data[\"text\"][0])\n",
    "print(\"Scam like email :\",data[\"text\"][12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔹 What I notice\n",
    "\n",
    "- **Columns:** `text` and `label`\n",
    "\n",
    "- **Content:**  \n",
    "  - `text` : the full email content (some are long, some short, some scam-like)  \n",
    "  - `label` : `1` for fraudulent/scam emails, `0` for legitimate emails\n",
    "\n",
    "- **Number of rows:** 1000 (reduced for development)\n",
    "\n",
    "- **Missing values:** none (`fillna(\"\")` was used to handle them)\n",
    "\n",
    "- **Example of a positive label (`1`) email:**  \n",
    "\n",
    "> DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL I AM MIKE CHUKWU …\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size     : 800\n",
      "Test set size         : 200\n",
      "Example training email:\n",
      " Dear=2C Good day hope fine=2Cdear am writting this mail with due respect and heartful of tears since we have not known or met ourselves previously I am asking for your assistance=2Ci have will be very glad if you can render me assistance to my situation now=2E I will make my proposal well known if I am given the opportunity=2E I would like to use this opportunity to introduce myself to you=2E Am Miss Johana Johnpaul 24 years old girl from Liberia =2Cthe only daughter of Late Godwin Johnpaul the deputy minister of national security under the leadership of president Charles Taylor of liberia who is now in exile after many innocent soul were killed=2E My father was killed by the government of CharlesTaylor=2Cheaccuse my father of coup attempt and after a month my mother Cynthia was also killed=2E The main reason why I am contacting you now is to seek for your assistance in the area of my future investment and also i want to hand over some huge amount of money to you=2E This money is Ten Millon Five Hundred Thousand US Dollars which was deposited some years ago by my father he made me the sole beneficiary=2Fnext of kin to the money=2E I am now asking you to stand on my behalf to make this claim for the bank=2Cam a girl and too young in age cann't handle this can of transaction=2Ci want you to stand as my foregin partner oversea and also to help me investment the money as well send me your telephone number and pictures ok=2E=2E=2E=2E I have all original copies of the documents concering this money with me here=2CSo please am expecting to hear from you soon=2E My Regards=2C Miss Johana   \n",
      "Example training label: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['text']      # emails\n",
    "y = data['label']     # labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y # Using stratify=y ensures the proportion of fraud/non-fraud emails is maintained in both sets.\n",
    ")\n",
    "\n",
    "print(\"Training set size     :\", len(X_train))\n",
    "print(\"Test set size         :\", len(X_test))\n",
    "print(\"Example training email:\\n\", X_train.iloc[0]) # 1st row\n",
    "# print(\"Example training email:\\n\", X_train.values[0]) 1st row value\n",
    "print(\"Example training label:\", y_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation   :  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "stopwords engl:  ['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Punctuation   : \",string.punctuation)\n",
    "print(\"stopwords engl: \",stopwords.words(\"english\")[100:110])\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL I AM MIKE CHUKWU , THE MANAGER, BILLS AND EXCHANGE AT THE FOREIGN REMITTANCE DEPARTMENT OF THE ZENITH INTERNATIONAL BANK PLC. I AM WRITING THIS LETTER TO ASK FOR YOUR SUPPORT AND COOPERATION TO CARRY OUT THIS BUSINESS OPPORTUNITY IN MY DEPARTMENT. WE DI\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def clean_email_from_html_code(text):\n",
    "    # Remove common HTML tags\n",
    "    text = text.replace(\"<br>\", \" \").replace(\"<BR>\", \" \")\n",
    "    text = text.replace(\"<p>\", \" \").replace(\"</p>\", \" \")\n",
    "    \n",
    "    # Remove HTML comments (very basic)\n",
    "    text = text.replace(\"<!--\", \" \").replace(\"-->\", \" \")\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sample_email = data['text'][0]\n",
    "cleaned_email = clean_email_from_html_code(sample_email)\n",
    "print(cleaned_email[:300])  # preview first 300 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      " Dear=2C Good day hope fine=2Cdear am writting this mail with due respect and heartful of tears since we have not known or met ourselves previously I am asking for your assistance=2Ci have will be very glad if you can render me assistance to my situation now=2E I will make my proposal well known if I am given the opportunity=2E I would like to use this opportunity to introduce myself to you=2E Am Miss Johana Johnpaul 24 years old girl from Liberia =2Cthe only daughter of Late Godwin Johnpaul the  ...\n",
      "\n",
      "Cleaned email:\n",
      " dear good day hope fine cdear am writting this mail with due respect and heartful of tears since we have not known or met ourselves previously am asking for your assistance ci have will be very glad if you can render me assistance to my situation now will make my proposal well known if am given the opportunity would like to use this opportunity to introduce myself to you am miss johana johnpaul years old girl from liberia cthe only daughter of late godwin johnpaul the deputy minister of national ...\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove special characters (keep only letters and spaces)\n",
    "    # Reason: This removes punctuation and symbols, leaving only letters and spaces.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # 2. Remove numbers\n",
    "    # Reason: Numbers are not needed for spam/ham classification in this lab.\n",
    "    # Although step 1 removed most digits, we include this for clarity.\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 3. Remove all single characters\n",
    "    # Reason: Words with a single character are usually noise ('I', 'a') and not informative.\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    \n",
    "    # 4. Remove single characters from the start\n",
    "    # Reason: If the first word is a single character, it may have remained and should be removed.\n",
    "    if len(words) > 0 and len(words[0]) == 1:\n",
    "        words = words[1:]\n",
    "    \n",
    "    # 5. Substitute multiple spaces with a single space\n",
    "    # Reason: Joining words after removing items may create extra spaces. This ensures clean spacing.\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # 6. Remove prefixed 'b' (like b'text')\n",
    "    # Reason: Some strings may come from byte representations; we remove a leading \"b'\" if present.\n",
    "    # In Python, a string that starts with b'...' is a byte string (type bytes), not a regular string (type str).\n",
    "    # byte_text = b'This is a sample email text.'\n",
    "    text = re.sub(r\"^b'\", '', text)\n",
    "    \n",
    "    # 7. Convert to lowercase\n",
    "    # Reason: Standardizes all text to lowercase to avoid treating \"Hello\" and \"hello\" differently.\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()  # remove any leading/trailing spaces\n",
    "\n",
    "# Example usage\n",
    "# sample_email = data['text'][0]\n",
    "# cleaned_email = clean_text(sample_email)\n",
    "# print(cleaned_email[:300])  # preview first 300 characters\n",
    "\n",
    "X_train_cleaned = X_train.apply(clean_text)\n",
    "X_test_cleaned  = X_test.apply(clean_text)\n",
    "\n",
    "# Test if it worked:\n",
    "\n",
    "# Original email\n",
    "original = X_train.iloc[0]\n",
    "\n",
    "# Cleaned email\n",
    "cleaned = X_train_cleaned.iloc[0]\n",
    "\n",
    "print(\"Original email:\\n\", original[:500], \"...\")   # first 500 chars\n",
    "print(\"\\nCleaned email:\\n\", cleaned[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Observations\n",
    "\n",
    "1. Removed special characters\n",
    "Original had things like `=2C` and `=2E` (which are HTML or URL-encoded commas and periods).  \n",
    "Cleaned version replaced them with spaces → exactly what we wanted.\n",
    "\n",
    "2. Removed numbers\n",
    "“24 years old” in original became “years old” → the number `24` was removed.\n",
    "3. Removed single characters\n",
    "Single letters or isolated characters like “I” at the start of sentences remain in some places because the regex keeps words of length >1. This is expected.\n",
    "4. Lowercased everything\n",
    "Original had “Dear”, “Miss Johana Johnpaul” → all lowercase in cleaned version.\n",
    "5. Removed extra spaces\n",
    "Original had many `=2C` and `=2E` sequences creating extra characters; cleaned text now has single spaces between words.\n",
    "6. Prefixed `b` check\n",
    "No `b'` prefixes appear, which is fine — your data didn’t\n",
    "\n",
    "By Sofia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopwords removal:\n",
      " dear good day hope fine cdear am writting this mail with due respect and heartful of tears since we have not known or met ourselves previously am asking for your assistance ci have will be very glad if you can render me assistance to my situation now will make my proposal well known if am given the \n",
      "\n",
      "After stopwords removal:\n",
      " dear good day hope fine cdear writting mail due respect heartful tears since known met previously asking assistance ci glad render assistance situation make proposal well known given opportunity would like use opportunity introduce miss johana johnpaul years old girl liberia cthe daughter late godwi\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to remove stopwords from ONE AND ONLY single string\n",
    "# my function has 2 arguments - NOT 1!\n",
    "# If it had 1 argument -> then later i could have called the function just by the name and it wouldn'train_test_split\n",
    "# be necessary to add the lambda in it\n",
    "# It's a thing that happens because of the pandas' function -> apply() <-\n",
    "def remove_stopwords(text, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# Referencing remove_stopwords without the 2 input arguments its better:\n",
    "# X_train_cleaned_no_stop = X_train_cleaned.apply(remove_stopwords) because you dont need the lambda\n",
    "X_train_cleaned_no_stop = X_train_cleaned.apply(lambda x: remove_stopwords(x, \"english\"))\n",
    "X_test_cleaned_no_stop  = X_test_cleaned.apply(lambda x: remove_stopwords(x, \"english\"))\n",
    "\n",
    "# Example: show first email before and after stopwords removal\n",
    "print(\"Before stopwords removal:\\n\", X_train_cleaned.iloc[0][:300])\n",
    "print(\"\\nAfter stopwords removal:\\n\", X_train_cleaned_no_stop.iloc[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔹 What I notice\n",
    "\n",
    "Most stopwords are removed: `am, this, we, have, not, to, my, will, if, can, me` are gone.\n",
    "\n",
    "Remaining words are more meaningful: `writting, mail, respect, assistance, proposal, johana, liberia`, etc.\n",
    "\n",
    "This is exactly what you want for NLP modeling, because the text now focuses on the important content words.\n",
    "\n",
    "By Sofia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to ./nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ./nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/sofiazogkza/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - './nltk_data'\n    - './nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_words)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Apply to your Series\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m X_train_lemmatized \u001b[38;5;241m=\u001b[39m X_train_cleaned_no_stop\u001b[38;5;241m.\u001b[39mapply(lemmatize_text)\n\u001b[1;32m     33\u001b[0m X_test_lemmatized  \u001b[38;5;241m=\u001b[39m X_test_cleaned_no_stop\u001b[38;5;241m.\u001b[39mapply(lemmatize_text)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Example: show first email\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlemmatize_text\u001b[39m(text):\n\u001b[1;32m     27\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# split string into words\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     lemmatized_words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(w, get_wordnet_pos(w)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_words)\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_wordnet_pos\u001b[39m(word):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     tag \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag([word])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()  \u001b[38;5;66;03m# first letter of POS tag\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     tag_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ,\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN,\n\u001b[1;32m     21\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB,\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV}\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag, wordnet\u001b[38;5;241m.\u001b[39mNOUN)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/tag/__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/tag/__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/tag/perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_from_json(lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/tag/perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaggers/averaged_perceptron_tagger_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/sofiazogkza/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - './nltk_data'\n    - './nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloads inside the venv folder\n",
    "nltk.download('wordnet', download_dir='./nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='./nltk_data')\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='./nltk_data')\n",
    "\n",
    "# Tell NLTK to look in this folder\n",
    "nltk.data.path.append('./nltk_data')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS helper function\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  # first letter of POS tag\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # default to noun\n",
    "\n",
    "# Function to lemmatize a single string\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()  # split string into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Apply to your Series\n",
    "X_train_lemmatized = X_train_cleaned_no_stop.apply(lemmatize_text)\n",
    "X_test_lemmatized  = X_test_cleaned_no_stop.apply(lemmatize_text)\n",
    "\n",
    "# Example: show first email\n",
    "print(\"Before lemmatization:\\n\", X_train_cleaned_no_stop.iloc[0][:300])\n",
    "print(\"\\nAfter lemmatization:\\n\", X_train_lemmatized.iloc[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m     10\u001b[0m X_ham \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(ham_emails)\n\u001b[0;32m---> 11\u001b[0m ham_word_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_ham\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     12\u001b[0m ham_words \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m     14\u001b[0m X_spam \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(spam_emails)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Step 1: Split the emails by label\n",
    "ham_emails = data[data['label'] == 0]['lemmatized_text']\n",
    "spam_emails = data[data['label'] == 1]['lemmatized_text']\n",
    "\n",
    "# Step 2: Fit and transform ham and then spam emails\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_ham = vectorizer.fit_transform(ham_emails)\n",
    "ham_word_counts = np.array(X_ham.sum(axis=0)).flatten()\n",
    "ham_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "X_spam = vectorizer.fit_transform(spam_emails)\n",
    "spam_word_counts = np.array(X_spam.sum(axis=0)).flatten()\n",
    "spam_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 3: Create a DataFrame with words and counts\n",
    "ham_df = pd.DataFrame({'word': ham_words, 'count': ham_word_counts})\n",
    "top10_ham = ham_df.sort_values(by='count', ascending=False).head(10)\n",
    "print(\"Top 10 words in HAM emails:\")\n",
    "print(top10_ham)\n",
    "\n",
    "spam_df = pd.DataFrame({'word': spam_words, 'count': spam_word_counts})\n",
    "top10_spam = spam_df.sort_values(by='count', ascending=False).head(10)\n",
    "print(\"\\nTop 10 words in SPAM emails:\")\n",
    "print(top10_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
